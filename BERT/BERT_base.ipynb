{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm,trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (WEIGHTS_NAME,\n",
    "                         BertConfig,\n",
    "                         BertModel,\n",
    "                         BertTokenizer,)\n",
    "\n",
    "from transformers import glue_compute_metrics as compute_metrics\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "\n",
    "from transformers.data.processors.utils import InputExample,DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter   #version 1.14 or higher\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import code\n",
    "import pickle\n",
    "from  sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger=logging.getLogger(__name__)\n",
    "MODEL_CLASSES={\n",
    "    \"bert\":(BertConfig,BertModel,BertTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #if args.n_gpu>0:\n",
    "    #    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAQProcessor(DataProcessor):\n",
    "    # def get_example_from_tensor_dict(self,tensor_dict):\n",
    "    #    return InputExample(\n",
    "    #        tensor_dict[\"idx\"].numpy(),\n",
    "    #        tensor_dict[\"sentence\"].numpy().decode(\"utf-8\"),\n",
    "    #        None,\n",
    "    #        str(tensor_dict[\"label\"].numpy()),\n",
    "    #    )\n",
    "    \n",
    "    def get_candidates(self,file_dir):\n",
    "        train_df=pd.read_csv(file_dir)\n",
    "        candidates=train_df[train_df[\"is_best\"]==1][[\"title\",\"reply\"]]\n",
    "        self.candidate_title=candidates[\"title\"].tolist()\n",
    "        self.candidate_reply=candidates[\"reply\"].tolist()\n",
    "        return self._create_examples(self.candidate_title,\"train\")\n",
    "    \n",
    "    def _create_examples(self,lines,set_type):\n",
    "        examples=[]\n",
    "        for (i,line) in enumerate(lines):\n",
    "            guid=\"%s-%s\" % (set_type,i)\n",
    "            examples.append(InputExample(guid=guid,text_a=line,text_b=None,label=1))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args,model,eval_dataset):\n",
    "    pooled_outputs=[]\n",
    "    \n",
    "    \n",
    "    eval_sampler=SequentialSampler(eval_dataset)\n",
    "    eval_dataloader=DataLoader(eval_dataset,sampler=eval_sampler,batch_size=16)\n",
    "    \n",
    "    \n",
    "    logger.info(\" Num examples = %d\",len(eval_dataset))\n",
    "    logger.info(\" Batch size = %d\",16)\n",
    "    eval_loss=0.0\n",
    "    nb_eval_steps=0\n",
    "    preds=None\n",
    "    ouut_label_ids=None\n",
    "    \n",
    "    for batch in tqdm(eval_dataloader,desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch=tuple(batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs={\"input_ids\": batch[0],\"attention_mask\":batch[1]}\n",
    "            if args[\"model_type\"]!=\"distilbert\":\n",
    "                inputs[\"token_type_ids\"]=(\n",
    "                    batch[2] if args[\"model_type\"] in [\"bert\",\"xlnet\"] else None\n",
    "                )\n",
    "            \n",
    "            outputs=model(**inputs)\n",
    "            # 关于这两个outputs\n",
    "            #\n",
    "            # 1.sequence_output 代表每个单词的向量,和ELMo出来的一样\n",
    "            #   size是: batch_size * seq_len * hidden_size, 32*句子里单词数量*768\n",
    "            #   在计算cosine similarity时, 也是可以用和ELMo方法一样,在seq_len上求平均\n",
    "            #   理论上这么做比用pooled_output好点,因为[CLS] token不太很能表示句子信息\n",
    "            #\n",
    "            # 2.pooled_output代表句子的向量\n",
    "            #   它其实是[CLS] tokens经过一个Linear层和一个tanh层的结果 \n",
    "            #   size是: batch_size * hidden_size,  32*768\n",
    "            sequence_output,pooled_output=outputs[:2]\n",
    "            pooled_outputs.append(pooled_output)\n",
    "            \n",
    "    return pooled_outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(args,task, tokenizer):\n",
    "    processor = FAQProcessor()\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(\n",
    "        args[\"data_dir\"],\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            list(filter(None, args[\"model_name_or_path\"].split(\"/\"))).pop(),\n",
    "            str(args[\"max_seq_length\"]),\n",
    "            str(task),\n",
    "        ),\n",
    "    )\n",
    "    logger.info(\"Creating features from dataset file at %s\", args[\"data_dir\"])\n",
    "    examples = (\n",
    "        processor.get_candidates(args[\"data_dir\"]) \n",
    "    )\n",
    "    features = convert_examples_to_features(\n",
    "        examples,\n",
    "        tokenizer,\n",
    "        label_list=[1],\n",
    "        output_mode=\"classification\",\n",
    "        max_length=args[\"max_seq_length\"],\n",
    "        pad_on_left=bool(args[\"model_type\"] in [\"xlnet\"]),  # pad on the left for xlnet\n",
    "        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "        pad_token_segment_id=4 if args[\"model_type\"] in [\"xlnet\"] else 0,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids)\n",
    "    return dataset, processor.candidate_title, processor.candidate_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "\n",
    "\n",
    "    set_seed()\n",
    "    task_name = \"\"\n",
    "    model_type = args[\"model_type\"]\n",
    "    \n",
    "    \n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "    config = config_class.from_pretrained(\n",
    "        args[\"config_name\"],\n",
    "        finetuning_task=task_name, \n",
    "        cache_dir=None,\n",
    "    )\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args[\"tokenizer_name\"],\n",
    "        do_lower_case=True,\n",
    "        cache_dir=None,\n",
    "    )\n",
    "    model = model_class.from_pretrained(\n",
    "        args[\"model_name_or_path\"],\n",
    "        from_tf=bool(\".ckpt\" in args[\"model_name_or_path\"]),\n",
    "        config=config,\n",
    "        cache_dir=None,\n",
    "    )\n",
    "\n",
    " \n",
    "    model.to(args[\"device\"])\n",
    "\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    eval_dataset, candidate_title, candidate_reply = load_examples(args, task_name, tokenizer)\n",
    "    \n",
    "    pooled_outputs = evaluate(args, model, eval_dataset)\n",
    "    \n",
    "    #把所有candidates embedding拼起来, 每个pooled_output是32, 拼起来一共 18677*768\n",
    "    candidate_embeddings = torch.cat([o.cpu().data for o in pooled_outputs]).numpy()\n",
    "\n",
    "    # code.interact(local=locals())\n",
    "\n",
    "    # load dataset\n",
    "    if not os.path.exists(\"embeddings.pkl\"):\n",
    "        with open(\"embeddings.pkl\", \"wb\") as fout:\n",
    "            pickle.dump([candidate_title, candidate_reply, candidate_embeddings], fout)\n",
    "    else:\n",
    "        with open(\"embeddings.pkl\", \"rb\") as fin:\n",
    "            candidate_title, candidate_reply, candidate_embeddings = pickle.load(fin)\n",
    "\n",
    "    while True:\n",
    "        title = input(\"你的问题是？\\n\")\n",
    "        if len(title.strip()) == 0:\n",
    "            continue\n",
    "\n",
    "        examples = [InputExample(guid=0, text_a=title, text_b=None, label=1)]\n",
    "        features = convert_examples_to_features(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            label_list=[1],\n",
    "            output_mode=\"classification\",\n",
    "            max_length=args[\"max_seq_length\"],\n",
    "            pad_on_left=bool(args[\"model_type\"] in [\"xlnet\"]),  # pad on the left for xlnet\n",
    "            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "            pad_token_segment_id=4 if args[\"model_type\"] in [\"xlnet\"] else 0,\n",
    "        )\n",
    "\n",
    "        # Convert to Tensors and build dataset\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids)\n",
    "        pooled_outputs = evaluate(args, model, dataset)\n",
    "        title_embedding = torch.cat([o.cpu().data for o in pooled_outputs]).numpy()\n",
    "\n",
    "        scores = cosine_similarity(title_embedding, candidate_embeddings)[0]\n",
    "        top5_indices = scores.argsort()[-5:][::-1]\n",
    "        for index in top5_indices:\n",
    "            print(\"可能的答案，参考问题：\" + candidate_title[index] + \"\\t答案：\" + candidate_reply[index] + \"\\t得分：\" + str(scores[index]))\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/02/2020 21:38:11 - INFO - transformers.configuration_utils -   loading configuration file /Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/config.json\n",
      "01/02/2020 21:38:11 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": \"\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "01/02/2020 21:38:11 - INFO - transformers.tokenization_utils -   Model name '/Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "01/02/2020 21:38:11 - INFO - transformers.tokenization_utils -   Didn't find file /Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "01/02/2020 21:38:11 - INFO - transformers.tokenization_utils -   Didn't find file /Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "01/02/2020 21:38:11 - INFO - transformers.tokenization_utils -   Didn't find file /Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n",
      "01/02/2020 21:38:11 - INFO - transformers.tokenization_utils -   loading file /Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/vocab.txt\n",
      "01/02/2020 21:38:11 - INFO - transformers.tokenization_utils -   loading file None\n",
      "01/02/2020 21:38:11 - INFO - transformers.tokenization_utils -   loading file None\n",
      "01/02/2020 21:38:11 - INFO - transformers.tokenization_utils -   loading file None\n",
      "01/02/2020 21:38:11 - INFO - transformers.modeling_utils -   loading weights file /Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/pytorch_model.bin\n",
      "01/02/2020 21:38:14 - INFO - __main__ -   Training/evaluation parameters {'model_type': 'bert', 'data_dir': '/Users/valleria_ruka/Desktop/FAQ/lawzhidao_filter.csv', 'model_name_or_path': '/Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/', 'config_name': '/Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/', 'tokenizer_name': '/Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/', 'do_train': False, 'do_eval': False, 'evaluate_during_training': False, 'do_lower_case': False, 'per_gpu_train_batch_size': 8, 'per_gpu_eval_batch_size': 8, 'gradient_accumulation_steps': 1, 'learning_rate': 5e-05, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'weight_decay': 0.0, 'max_seq_length': 128}\n",
      "01/02/2020 21:38:14 - INFO - __main__ -   Creating features from dataset file at /Users/valleria_ruka/Desktop/FAQ/lawzhidao_filter.csv\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   Writing example 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   guid: train-0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   input_ids: 101 1762 3791 2526 704 2137 7032 680 6370 7032 4638 1277 1166 6370 7032 1469 2137 7032 1525 702 1358 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   guid: train-1\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   input_ids: 101 4668 4961 5389 4638 4306 5389 2145 860 3221 784 720 8024 4668 4961 5389 4638 4306 5389 712 860 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   guid: train-2\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   input_ids: 101 7478 3791 2544 3146 2501 3322 3354 3354 2768 7478 3791 5307 5852 5389 1408 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   guid: train-3\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   input_ids: 101 1057 2147 2898 1143 6121 1136 839 782 5543 679 5543 1161 1152 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   guid: train-4\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   input_ids: 101 2190 769 6858 752 3125 6569 818 6371 2137 741 679 3302 2582 720 1215 8024 769 6858 752 3125 2938 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 21:38:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n",
      "01/02/2020 21:38:17 - INFO - transformers.data.processors.glue -   Writing example 10000\n",
      "01/02/2020 21:38:19 - INFO - __main__ -    Num examples = 18243\n",
      "01/02/2020 21:38:19 - INFO - __main__ -    Batch size = 16\n",
      "Evaluating: 100%|██████████| 1141/1141 [58:42<00:00,  3.09s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的问题是？\n",
      "汽车超速怎么办?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/02/2020 22:43:16 - INFO - transformers.data.processors.glue -   Writing example 0\n",
      "01/02/2020 22:43:16 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "01/02/2020 22:43:16 - INFO - transformers.data.processors.glue -   guid: 0\n",
      "01/02/2020 22:43:16 - INFO - transformers.data.processors.glue -   input_ids: 101 3749 6756 6631 6862 2582 720 1215 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 22:43:16 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 22:43:16 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 22:43:16 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n",
      "01/02/2020 22:43:16 - INFO - __main__ -    Num examples = 1\n",
      "01/02/2020 22:43:16 - INFO - __main__ -    Batch size = 16\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可能的答案，参考问题：遇到车祸，肇事车辆逃逸后，怎么处理\t答案：所谓交通事故逃逸是指行为人在发生交通事故后，为逃避法律追究而逃跑的行为。主观上是为了逃避法律责任的追究。交通事故逃逸的，具体处理如下：1、交通肇事后逃逸或者有其他特别恶劣情节的，处3年以上7年以下有期徒刑。所谓“交通肇事后逃逸”，《解释》第3条规定，是指行为人具有本解释第2条第1款规定和第2款第(1)至(5)项规定的情形之一，在发生交通事故后，为逃避法律追究而逃跑的行为。这里要注意对“交通肇事后逃逸”的认定，首先，交通肇事逃逸的前提条件是“为逃避法律追究”，其次，交通肇事逃逸并没有时间和场所的限定，不应仅理解为“逃离事故现场”，对于肇事后未逃离(或未能逃离)事故现场，而是在将伤者送至医院后或者等待交通管理部门处理的时候逃跑的，也应视为“交通肇事后逃逸”。所谓“其他特别恶劣情节”，《解释》第4条规定：交通肇事具有下列情形之一的，属于“有其他特别恶劣情节”：(1)死亡二人以上或者重伤五人以上，负事故全部或者主要责任的;(2)死亡六人以上，负事故同等责任的;(3)造成公共财产或者他人财产直接损失，负事故全部或者主要责任，无能力赔偿数额在六十万元以上的。2、因交通肇事逃逸致人死亡的，处7年以上有期徒刑。根据《解释》，“因交通肇事逃逸致人死亡”，是指行为人在交通肇事后为逃避法律追究而逃跑，致使被害人因得不到救助而死亡的情形。但刑法理论上对“因交通肇事逃逸致人死亡”形成了诸多不同的观点。本书认为，“因交通肇事逃逸致人死亡”，的心理态度应限于过失，因为交通肇事罪是一种过失犯罪，为保持犯罪构成的纯洁性，其加重构成的心理态度也应是过失。故《解释》规定：行为人在交通肇事后为逃避法律追究，将被害人带离事故现场后隐藏或者遗弃，致使被害人无法得到救助而死亡或者严重残疾的，应当分别依照刑法第232条、第234条第2款的规定，以故意杀人罪或者故意伤害罪定罪处罚。\t得分：0.99053067\n",
      "\n",
      "可能的答案，参考问题：交通事故被人撞了怎么办？\t答案：这种情况是交通事故引发的经济纠纷.首先需要拿到交警的事故责任认定书，然后确定双方的责任比例，比如说主要责任次要责任（比如说责任比例是三七开，即主要责任的是7，次要责任是3），按照这个比例，你方的费用对方需要承担70%，你需要收集你的医药费收据票据等，要求对方赔付，如果对方不赔付，你可以收集这些依据，向人*法*提出*事诉讼，要求肇事方赔偿。\t得分：0.98974323\n",
      "\n",
      "可能的答案，参考问题：法院法官错判不作为怎么办?\t答案：可以通过上诉或者申诉请求改判，但必须有证据支持，否则不会如愿。\t得分：0.9867459\n",
      "\n",
      "可能的答案，参考问题：如果公司拖欠工资拖了一个月，那该怎么办\t答案：1、可以同单位协商解决。；2、如果确实不能协商解决，单位又拖欠工资不发，你可以去当地劳动监察部门投诉，要求立案处理；3、也可以直接申请仲裁；4、如果对仲裁结果不满意可以在拿到仲裁书后15天之内到法院起诉；\t得分：0.9863821\n",
      "\n",
      "可能的答案，参考问题：高利贷是否属于不当得利，债已还清，还能打官\t答案：民间借贷是一种民事法律行为合法的在法律上认定的高利。贷是指超过银行同期基准利率的4倍，也就是产生法律纠纷时支持同期基准利率四倍以内的利率水平，超过银行同期基准利率的4倍，即为高利。贷，不受法律保护。因为不同期限的基准利率不同，高利，贷的法律认定还与期限相关。同期基准利率是指中央银行公布的基准利率，如一年期，基准利率为6%，4倍为24%，即超过24%为法律认定的高利，贷。就一年期来说，1####元，年利息不超过24%*1####=24##元不算高，利贷。\t得分：0.9862101\n",
      "\n",
      "你的问题是？\n",
      "怎么样找律师?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/02/2020 22:44:00 - INFO - transformers.data.processors.glue -   Writing example 0\n",
      "01/02/2020 22:44:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "01/02/2020 22:44:00 - INFO - transformers.data.processors.glue -   guid: 0\n",
      "01/02/2020 22:44:00 - INFO - transformers.data.processors.glue -   input_ids: 101 2582 720 3416 2823 2526 2360 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 22:44:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 22:44:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/02/2020 22:44:00 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n",
      "01/02/2020 22:44:00 - INFO - __main__ -    Num examples = 1\n",
      "01/02/2020 22:44:00 - INFO - __main__ -    Batch size = 16\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可能的答案，参考问题：工资保密协议如何写?\t答案：举例如下：工资保密协议甲方：乙方：为了保护甲乙双方的权益，保守双方的薪酬秘密，维护和谐的工作氛围，促进双方的稳定发展，甲、乙双方在遵循诚实信用、平等自愿的原则下，经协商就薪酬保密事项达成协议，条款如下：一、甲方的工资薪酬体系设置属于甲方的管理信息内容之一，并经甲方采取保密措施，属于甲方的商业秘密。乙方所接触的工资薪酬信息未经甲方允许不得向第三方透露。乙方必须严格遵守甲方的保密制度，不得询问与自己薪酬无关的情况，不得泄漏本人的薪酬待遇情况，不得以任何形式向他人询问薪酬状况。非本人原因知悉他人薪酬情况者，应及时向甲方主管报告，并采取措施防止泄密进一步扩大，不得传播、散步、比对。二、若乙方作为管理人员及相关工作人员参与或接触薪酬管理或工资作业，要严格遵守公司薪酬保密制度，不得向第三方泄漏所知悉的工资薪酬情况，不得在任何非作业场所公开谈论与薪酬有关的事宜，不得在与薪酬作业无关的人员面前进行操作。带有工资信息的资料必须妥善保管，需存放入文件柜或抽屉，不得随意乱放。废弃不用的资料信息，需用粉碎机进行粉碎处理。三、甲方要对乙方的工资薪酬情况进行保密，非因履行工作职责之需要不得向第三方透露。四、违约责任约定：本协议约定的工资保密事项是公司的重要纪律。违反本协议约定的，一经核实给予违纪解除劳动合同的处理。五、本协议正本一式两份，甲乙双方各执一份，作为劳动合同附件，与劳动合同具有同等法律效力。本协议经甲、乙双方签字或盖章之日起生效。甲方（盖章或签字）：乙方（签字）：日期：年月日日期：年月日\t得分：0.990789\n",
      "\n",
      "可能的答案，参考问题：要怎样才能申请法律援助\t答案：根据《法律援助条例》相关规定，申请法律援助，要符合援助范围及所需材料。《法律援助条例》第十条公*对下列需要代理的事项，因经济困难没有委托代理人的，可以向法律援助机构申请法律援助：（一）依法请求国家赔偿的；（二）请求给予社会保险待遇或者最低生活保障待遇的；（三）请求发给抚恤金、救济金的；（四）请求给付赡养费、抚养费、扶养费的；（五）请求支付劳动报酬的；（六）主张因见义勇为行为产生的*事权益的。省、自治区、直辖市人*政府可以对前款规定以外的法律援助事项作出补充规定。公*可以就本条第一款、第二款规定的事项向法律援助机构申请法律咨询。第十一条刑事诉讼中有下列情形之一的，公*可以向法律援助机构申请法律援助：（一）犯罪嫌疑人在被侦查机关第一次讯问后或者采取强制措施之日起，因经济困难没有聘请律师的；（二）公诉案件中的被害人及其法定代理人或者近亲属，自案件移送审查起诉之日起，因经济困难没有委托诉讼代理人的；（三）自诉案件的自诉人及其法定代理人，自案件被人*法*受理之日起，因经济困难没有委托诉讼代理人的。第十二条公诉人出庭公诉的案件，被告人因经济困难或者其他原因没有委托辩护人，人*法*为被告人指定辩护时，法律援助机构应当提供法律援助。被告人是盲、聋、哑人或者未成年人而没有委托辩护人的，或者被告人可能被判处死刑而没有委托辩护人的，人*法*为被告人指定辩护时，法律援助机构应当提供法律援助，无须对被告人进行经济状况的审查。第十七条公*申请代理、刑事辩护的法律援助应当提交下列证件、证明材料：（一）身份证或者其他有效的身份证明，代理申请人还应当提交有代理权的证明；（二）经济困难的证明；（三）与所申请法律援助事项有关的案件材料。申请应当采用书面形式，填写申请表；以书面形式提出申请确有困难的，可以口头申请，由法律援助机构工作人员或者代为转交申请的有关机构工作人员作书面记录。\t得分：0.9904959\n",
      "\n",
      "可能的答案，参考问题：笔迹鉴定怎么收费的?\t答案：每个省和每个地方的收费标准都不一样。都是有自己所在省司法厅和发改委指导定价。都是有收费明细的。而且笔迹鉴定也分很多种：变造伪造篡改、后期添加等等。\t得分：0.9900465\n",
      "\n",
      "可能的答案，参考问题：请律师怎么收费？\t答案：各地区有一定差异，大致如下：无财产争议案件：普通民事、经济、行政案件，不涉及财产的，根据案件性质、复杂程度、工作所需耗费时间等因素，在6###—1#####元之间协商收取；外地民事、经济、行政案件不涉及财产的，代理费不低于2####元；律师代理分为以下几种第一种：一般代理。依律师职业道德为当事人代理案件，维护当事人权利。一般为纠纷标的的3-5%。标的高的话也可以低于该比例。第二种：风险代理。按胜诉金额或以得到的金额的百分比支付代理费，或按减少的支付额的百分比支付代理费，比例当然高于一般代理。第三种：半风险代理。给一定的基本费用，其他的再按风险代理计算，当然就介于一般代理和风险代理的比例之间。\t得分：0.98818046\n",
      "\n",
      "可能的答案，参考问题：请律师有多少钱\t答案：各地区有一定差异，大致如下：无财产争议案件：普通民事、经济、行政案件，不涉及财产的，根据案件性质、复杂程度、工作所需耗费时间等因素，在6###—1#####元之间协商收取；外地民事、经济、行政案件不涉及财产的，代理费不低于2####元；律师代理分为以下几种第一种：一般代理。依律师职业道德为当事人代理案件，维护当事人权利。一般为纠纷标的的3-5%。标的高的话也可以低于该比例。第二种：风险代理。按胜诉金额或以得到的金额的百分比支付代理费，或按减少的支付额的百分比支付代理费，比例当然高于一般代理。第三种：半风险代理。给一定的基本费用，其他的再按风险代理计算，当然就介于一般代理和风险代理的比例之间。\t得分：0.9879018\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/my_allennlp/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_allennlp/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_allennlp/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_allennlp/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0ea773397ce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-b5774ad8e89b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"你的问题是？\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_allennlp/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_allennlp/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args={\n",
    "    \"model_type\":\"bert\",\n",
    "    \"data_dir\": \"/Users/valleria_ruka/Desktop/FAQ/lawzhidao_filter.csv\",\n",
    "    \"model_name_or_path\": \"/Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/\",\n",
    "    \"config_name\": \"/Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/\",\n",
    "    \"tokenizer_name\": \"/Users/valleria_ruka/Desktop/FAQ/BERT/chinese_wwm_ext_pytorch/\",\n",
    "    \"do_train\":False,\n",
    "    \"do_eval\":False,\n",
    "    \"evaluate_during_training\":False,\n",
    "    \"do_lower_case\":False,\n",
    "    \"per_gpu_train_batch_size\":8,\n",
    "    \"per_gpu_eval_batch_size\":8,\n",
    "    \"gradient_accumulation_steps\":1,\n",
    "    \"learning_rate\":5e-5,\n",
    "    \"adam_epsilon\":1e-8,\n",
    "    \"max_grad_norm\":1.0,\n",
    "    \"weight_decay\":0.0,\n",
    "    \"max_grad_norm\":1.0,\n",
    "    \"max_seq_length\":128,\n",
    "    \"device\":\"cpu\",\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args_eval[\"device\"]=\"cuda\"\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
