{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"BERT_base.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"aU2v64E7jrD8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":598},"outputId":"4f81bdb0-5f37-4084-9b23-24b32a83fade","executionInfo":{"status":"ok","timestamp":1579500397010,"user_tz":-480,"elapsed":6800,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}}},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n","\r\u001b[K     |▊                               | 10kB 18.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 3.3MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 3.2MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n","\u001b[K     |████████████████████████████████| 870kB 58.5MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 51.1MB/s \n","\u001b[?25hRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=2352442439db053eaad2a666ae8dc974d9bbca33e9ac5ba585d5b46dde40a79d\n","  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6V1Yrh8-jmIQ","colab_type":"code","colab":{}},"source":["import argparse\n","import glob\n","import json\n","import logging\n","import os\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a88i40mYjmIW","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.utils.data import DataLoader,RandomSampler,SequentialSampler,TensorDataset\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm,trange"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qe1JdkhTjmIY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"e41e73e4-3389-43d4-f6df-93dbe62aaedd","executionInfo":{"status":"ok","timestamp":1579500409794,"user_tz":-480,"elapsed":2215,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}}},"source":["from transformers import (WEIGHTS_NAME,\n","                         BertConfig,\n","                         BertModel,\n","                         BertTokenizer,)\n","\n","from transformers import glue_compute_metrics as compute_metrics\n","from transformers import glue_convert_examples_to_features as convert_examples_to_features\n","from transformers import glue_output_modes as output_modes\n","from transformers import glue_processors as processors\n","\n","from transformers.data.processors.utils import InputExample,DataProcessor"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"O0IXFWdUjmIb","colab_type":"code","colab":{}},"source":["try:\n","    from torch.utils.tensorboard import SummaryWriter   #version 1.14 or higher\n","except ImportError:\n","    from tensorboardX import SummaryWriter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNUJubUPjmId","colab_type":"code","colab":{}},"source":["import code\n","import pickle\n","from  sklearn.metrics.pairwise import cosine_similarity"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-LZo65ikDP4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"e0fd57c4-2d37-428b-bb1a-2b7d0af0c8ca","executionInfo":{"status":"ok","timestamp":1579500474771,"user_tz":-480,"elapsed":21650,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NUEsVDZBjmIf","colab_type":"code","colab":{}},"source":["logger=logging.getLogger(__name__)\n","MODEL_CLASSES={\n","    \"bert\":(BertConfig,BertModel,BertTokenizer),\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ngJ8bzIjmIi","colab_type":"code","colab":{}},"source":["def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    #if args.n_gpu>0:\n","    #    torch.cuda.manual_seed_all(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCU2Slg_jmIk","colab_type":"code","colab":{}},"source":["class FAQProcessor(DataProcessor):\n","    # def get_example_from_tensor_dict(self,tensor_dict):\n","    #    return InputExample(\n","    #        tensor_dict[\"idx\"].numpy(),\n","    #        tensor_dict[\"sentence\"].numpy().decode(\"utf-8\"),\n","    #        None,\n","    #        str(tensor_dict[\"label\"].numpy()),\n","    #    )\n","    \n","    def get_candidates(self,file_dir):\n","        train_df=pd.read_csv(file_dir)\n","        candidates=train_df[train_df[\"is_best\"]==1][[\"title\",\"reply\"]]\n","        self.candidate_title=candidates[\"title\"].tolist()\n","        self.candidate_reply=candidates[\"reply\"].tolist()\n","        return self._create_examples(self.candidate_title,\"train\")\n","    \n","    def _create_examples(self,lines,set_type):\n","        examples=[]\n","        for (i,line) in enumerate(lines):\n","            guid=\"%s-%s\" % (set_type,i)\n","            examples.append(InputExample(guid=guid,text_a=line,text_b=None,label=1))\n","        return examples"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WuOzoQpHjmIm","colab_type":"code","colab":{}},"source":["def evaluate(args,model,eval_dataset):\n","    pooled_outputs=[]\n","    \n","    \n","    eval_sampler=SequentialSampler(eval_dataset)\n","    eval_dataloader=DataLoader(eval_dataset,sampler=eval_sampler,batch_size=16)\n","    \n","    \n","    logger.info(\" Num examples = %d\",len(eval_dataset))\n","    logger.info(\" Batch size = %d\",16)\n","    eval_loss=0.0\n","    nb_eval_steps=0\n","    preds=None\n","    ouut_label_ids=None\n","    \n","    for batch in tqdm(eval_dataloader,desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(args[\"device\"]) for t in batch)\n","        \n","        with torch.no_grad():\n","            inputs={\"input_ids\": batch[0],\"attention_mask\":batch[1]}\n","            if args[\"model_type\"]!=\"distilbert\":\n","                inputs[\"token_type_ids\"]=(\n","                    batch[2] if args[\"model_type\"] in [\"bert\",\"xlnet\"] else None\n","                )\n","            \n","            outputs=model(**inputs)\n","            # 关于这两个outputs\n","            #\n","            # 1.sequence_output 代表每个单词的向量,和ELMo出来的一样\n","            #   size是: batch_size * seq_len * hidden_size, 32*句子里单词数量*768\n","            #   在计算cosine similarity时, 也是可以用和ELMo方法一样,在seq_len上求平均\n","            #   理论上这么做比用pooled_output好点,因为[CLS] token不太很能表示句子信息\n","            #\n","            # 2.pooled_output代表句子的向量\n","            #   它其实是sequence output的第一个token(即[CLS])经过一个Linear层和一个tanh层的结果 \n","            #   size是: batch_size * hidden_size,  32*768\n","            sequence_output,pooled_output=outputs[:2]\n","            pooled_outputs.append(pooled_output)\n","            \n","    return pooled_outputs  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1z9uc0_jmIo","colab_type":"code","colab":{}},"source":["def load_examples(args,task, tokenizer):\n","    processor = FAQProcessor()\n","    # Load data features from cache or dataset file\n","    cached_features_file = os.path.join(\n","        args[\"data_dir\"],\n","        \"cached_{}_{}_{}\".format(\n","            list(filter(None, args[\"model_name_or_path\"].split(\"/\"))).pop(),\n","            str(args[\"max_seq_length\"]),\n","            str(task),\n","        ),\n","    )\n","    logger.info(\"Creating features from dataset file at %s\", args[\"data_dir\"])\n","    examples = (\n","        processor.get_candidates(args[\"data_dir\"]) \n","    )\n","    features = convert_examples_to_features(\n","        examples,\n","        tokenizer,\n","        label_list=[1],\n","        output_mode=\"classification\",\n","        max_length=args[\"max_seq_length\"],\n","        pad_on_left=bool(args[\"model_type\"] in [\"xlnet\"]),  # pad on the left for xlnet\n","        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","        pad_token_segment_id=4 if args[\"model_type\"] in [\"xlnet\"] else 0,\n","    )\n","\n","\n","    # Convert to Tensors and build dataset\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n","\n","    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids)\n","    return dataset, processor.candidate_title, processor.candidate_reply"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvBoqsUkjmIr","colab_type":"code","colab":{}},"source":["def main(args):\n","    \n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n","    )\n","\n","\n","    set_seed()\n","    task_name = \"\"\n","    model_type = args[\"model_type\"]\n","    \n","    \n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n","    config = config_class.from_pretrained(\n","        args[\"config_name\"],\n","        finetuning_task=task_name, \n","        cache_dir=None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args[\"tokenizer_name\"],\n","        do_lower_case=True,\n","        cache_dir=None,\n","    )\n","    model = model_class.from_pretrained(\n","        args[\"model_name_or_path\"],\n","        from_tf=bool(\".ckpt\" in args[\"model_name_or_path\"]),\n","        config=config,\n","        cache_dir=None,\n","    )\n","\n"," \n","    model.to(args[\"device\"])\n","\n","    # load dataset\n","    if not os.path.exists(\"/content/drive/My Drive/embeddings.pkl\"):\n","        logger.info(\"Training/evaluation parameters %s\", args)\n","\n","        eval_dataset, candidate_title, candidate_reply = load_examples(args, task_name, tokenizer)\n","    \n","        pooled_outputs = evaluate(args, model, eval_dataset)\n","    \n","        #把所有candidates embedding拼起来, 每个pooled_output是32, 拼起来一共 18677*768\n","        candidate_embeddings = torch.cat([o.cpu().data for o in pooled_outputs]).numpy()\n","\n","        with open(\"/content/drive/My Drive/embeddings.pkl\", \"wb\") as fout:\n","            pickle.dump([candidate_title, candidate_reply, candidate_embeddings], fout)\n","\n","            \n","    else:\n","        with open(\"/content/drive/My Drive/embeddings.pkl\", \"rb\") as fin:\n","            candidate_title, candidate_reply, candidate_embeddings = pickle.load(fin)\n","\n","    while True:\n","        title = input(\"你的问题是？\\n\")\n","        if len(title.strip()) == 0:\n","            continue\n","        \n","        #[CLS]句子[SEP]None[SEP] 只用了一个句子\n","        examples = [InputExample(guid=0, text_a=title, text_b=None, label=1)]\n","        \n","        \n","        features = convert_examples_to_features(\n","            examples,\n","            tokenizer,\n","            label_list=[1],\n","            output_mode=\"classification\",\n","            max_length=args[\"max_seq_length\"],\n","            pad_on_left=bool(args[\"model_type\"] in [\"xlnet\"]),  # pad on the left for xlnet\n","            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","            pad_token_segment_id=4 if args[\"model_type\"] in [\"xlnet\"] else 0,\n","        )\n","\n","        # Convert to Tensors and build dataset\n","        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","        all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n","\n","        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids)\n","        pooled_outputs = evaluate(args, model, dataset)\n","        title_embedding = torch.cat([o.cpu().data for o in pooled_outputs]).numpy()\n","\n","        scores = cosine_similarity(title_embedding, candidate_embeddings)[0]\n","        top5_indices = scores.argsort()[-5:][::-1]\n","        for index in top5_indices:\n","            print(\"可能的答案，参考问题：\" + candidate_title[index] + \"\\t答案：\" + candidate_reply[index] + \"\\t得分：\" + str(scores[index]))\n","            print()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9jjpo0qjmIu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7a579e1f-a450-478a-dd7d-fb6bca60b639","executionInfo":{"status":"error","timestamp":1579507358797,"user_tz":-480,"elapsed":1045014,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}}},"source":["args={\n","    \"model_type\":\"bert\",\n","    \"data_dir\": \"/content/drive/My Drive/Data/lawzhidao_filter.csv\",\n","    \"model_name_or_path\": \"/content/drive/My Drive/chinese_wwm_ext_pytorch/\",\n","    \"config_name\": \"/content/drive/My Drive/chinese_wwm_ext_pytorch/\",\n","    \"tokenizer_name\": \"/content/drive/My Drive/chinese_wwm_ext_pytorch/\",\n","    \"do_train\":False,\n","    \"do_eval\":False,\n","    \"evaluate_during_training\":False,\n","    \"do_lower_case\":False,\n","    \"per_gpu_train_batch_size\":8,\n","    \"per_gpu_eval_batch_size\":8,\n","    \"gradient_accumulation_steps\":1,\n","    \"learning_rate\":5e-5,\n","    \"adam_epsilon\":1e-8,\n","    \"max_grad_norm\":1.0,\n","    \"weight_decay\":0.0,\n","    \"max_grad_norm\":1.0,\n","    \"max_seq_length\":128,\n","    \"device\":\"cpu\",\n","    \n","    \n","}\n","\n","\n","if torch.cuda.is_available():\n","    args[\"device\"]=\"cuda\"\n","\n","main(args)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["01/20/2020 07:45:13 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/chinese_wwm_ext_pytorch/config.json\n","01/20/2020 07:45:13 - INFO - transformers.configuration_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"finetuning_task\": \"\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 21128\n","}\n","\n","01/20/2020 07:45:13 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/chinese_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/My Drive/chinese_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n","01/20/2020 07:45:13 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/chinese_wwm_ext_pytorch/added_tokens.json. We won't load it.\n","01/20/2020 07:45:13 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/chinese_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n","01/20/2020 07:45:13 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/chinese_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n","01/20/2020 07:45:13 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/chinese_wwm_ext_pytorch/vocab.txt\n","01/20/2020 07:45:13 - INFO - transformers.tokenization_utils -   loading file None\n","01/20/2020 07:45:13 - INFO - transformers.tokenization_utils -   loading file None\n","01/20/2020 07:45:13 - INFO - transformers.tokenization_utils -   loading file None\n","01/20/2020 07:45:13 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/chinese_wwm_ext_pytorch/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-04496745b57c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-29-b881cc559b8a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"你的问题是？\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"XCxEOBRkjmI3","colab_type":"code","colab":{}},"source":["def mean_reciprocal_rank(rs):\n","    rs = (np.asarray(r).nonzero()[0] for r in rs)\n","    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n","\n","\n","def evaluate_mmr(args):\n","    # load model\n","    set_seed()\n","    task_name = \"\"\n","    model_type = args[\"model_type\"]\n","    \n","    \n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n","    config = config_class.from_pretrained(\n","        args[\"config_name\"],\n","        finetuning_task=task_name, \n","        cache_dir=None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args[\"tokenizer_name\"],\n","        do_lower_case=True,\n","        cache_dir=None,\n","    )\n","    model = model_class.from_pretrained(\n","        args[\"model_name_or_path\"],\n","        from_tf=bool(\".ckpt\" in args[\"model_name_or_path\"]),\n","        config=config,\n","        cache_dir=None,\n","    )\n","    model.to(args[\"device\"])\n","    \n","    \n","    \n","    # load candidate embeddings\n","    with open(\"/content/drive/My Drive/embeddings.pkl\", \"rb\") as fin:\n","        candidate_title, candidate_reply, candidate_embeddings = pickle.load(fin)\n","    \n","    \n","    # load test data\n","    df=pd.read_csv(args[\"data_dir\"])\n","    questions=df[\"question\"].tolist()\n","    matched_questions=df[\"title\"].tolist()\n","    matched_questions_index = []\n","    for q in matched_questions:\n","        flg = False\n","        for i, _q in enumerate(candidate_title):\n","            if q == _q:\n","                matched_questions_index.append([i])\n","                flg = True\n","                break\n","        if flg == False:\n","            matched_questions_index.append([-1])\n","    \n","    matched_questions_index = np.asarray(matched_questions_index)\n","    \n","    \n","    #convert questions in test data to BERT input\n","    examples = [InputExample(guid=0, text_a=title, text_b=None, label=1) for title in questions]\n","\n","    \n","    features = convert_examples_to_features(\n","        examples,\n","        tokenizer,\n","        label_list=[1],\n","        output_mode=\"classification\",\n","        max_length=args[\"max_seq_length\"],\n","        pad_on_left=bool(args[\"model_type\"] in [\"xlnet\"]),  # pad on the left for xlnet\n","        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","        pad_token_segment_id=4 if args[\"model_type\"] in [\"xlnet\"] else 0,\n","    )\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n","\n","\n","    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids)\n","    sequence_outputs = evaluate(args, model, dataset)\n","    question_embedding = torch.cat([o.cpu() for o in sequence_outputs]).numpy()\n","\n","\n","    \n","    \n","    scores = cosine_similarity(question_embedding, candidate_embeddings)\n","    sorted_indices = scores.argsort()[:, ::-1]#[-5:][::-1]\n","    # code.interact(local=locals())\n","    mmr = mean_reciprocal_rank(sorted_indices==matched_questions_index)\n","    print(\"mean reciprocal rank: {}\".format(mmr))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qg1JtcuZjmI5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"17cfffb9-e04e-4cfa-bd14-8ab7a1adcd96","executionInfo":{"status":"ok","timestamp":1579507776164,"user_tz":-480,"elapsed":4173,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}}},"source":["args={\n","    \"model_type\":\"bert\",\n","    \"data_dir\": \"/content/drive/My Drive/Data/lawzhidao_evaluate.csv\",\n","    \"model_name_or_path\": \"/content/drive/My Drive/chinese_wwm_ext_pytorch/\",\n","    \"config_name\": \"/content/drive/My Drive/chinese_wwm_ext_pytorch/\",\n","    \"tokenizer_name\": \"/content/drive/My Drive/chinese_wwm_ext_pytorch/\",\n","    \"do_train\":False,\n","    \"do_eval\":False,\n","    \"evaluate_during_training\":False,\n","    \"do_lower_case\":False,\n","    \"per_gpu_train_batch_size\":8,\n","    \"per_gpu_eval_batch_size\":8,\n","    \"gradient_accumulation_steps\":1,\n","    \"learning_rate\":5e-5,\n","    \"adam_epsilon\":1e-8,\n","    \"max_grad_norm\":1.0,\n","    \"weight_decay\":0.0,\n","    \"max_grad_norm\":1.0,\n","    \"max_seq_length\":128,\n","    \"device\":\"cpu\",\n","    \n","    \n","}\n","\n","\n","if torch.cuda.is_available():\n","    args[\"device\"]=\"cuda\"\n","    \n","    \n","evaluate_mmr(args)"],"execution_count":44,"outputs":[{"output_type":"stream","text":["01/20/2020 08:09:31 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/chinese_wwm_ext_pytorch/config.json\n","01/20/2020 08:09:31 - INFO - transformers.configuration_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"finetuning_task\": \"\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 21128\n","}\n","\n","01/20/2020 08:09:31 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/chinese_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/My Drive/chinese_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n","01/20/2020 08:09:31 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/chinese_wwm_ext_pytorch/added_tokens.json. We won't load it.\n","01/20/2020 08:09:32 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/chinese_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n","01/20/2020 08:09:32 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/chinese_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n","01/20/2020 08:09:32 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/chinese_wwm_ext_pytorch/vocab.txt\n","01/20/2020 08:09:32 - INFO - transformers.tokenization_utils -   loading file None\n","01/20/2020 08:09:32 - INFO - transformers.tokenization_utils -   loading file None\n","01/20/2020 08:09:32 - INFO - transformers.tokenization_utils -   loading file None\n","01/20/2020 08:09:32 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/chinese_wwm_ext_pytorch/pytorch_model.bin\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   Writing example 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   *** Example ***\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   guid: 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   input_ids: 101 982 2864 1166 782 4638 4212 4275 117 3221 1415 909 4306 5494 1008 3326 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   *** Example ***\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   guid: 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   input_ids: 101 6435 1152 752 2526 2360 1914 2208 7178 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   *** Example ***\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   guid: 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   input_ids: 101 1957 3175 2582 720 6629 6401 4895 2042 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   *** Example ***\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   guid: 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   input_ids: 101 6566 965 6814 1914 1377 809 5632 7674 1408 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   *** Example ***\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   guid: 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   input_ids: 101 2339 6598 2870 3612 2582 720 1215 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","01/20/2020 08:09:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 0)\n","01/20/2020 08:09:34 - INFO - __main__ -    Num examples = 50\n","01/20/2020 08:09:34 - INFO - __main__ -    Batch size = 16\n","Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["mean reciprocal rank: 0.1838426239890649\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hLnKJie9jmI7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}