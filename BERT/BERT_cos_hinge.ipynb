{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_cos_hinge.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNbOlwP9YEUg7s9d4kE1v3z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DDNOwtELHp3g","colab_type":"code","outputId":"9270b3bd-d83a-4a2c-f0a7-987aaf59866e","executionInfo":{"status":"ok","timestamp":1581083503367,"user_tz":-480,"elapsed":2769,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch\n","torch.cuda.is_available()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"UKprJBAXWSPl","colab_type":"code","outputId":"5e045b6e-e7c5-4925-a54a-75154c36ec7a","executionInfo":{"status":"ok","timestamp":1581083516361,"user_tz":-480,"elapsed":15742,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}},"colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.4.1)\n","Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.9)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.6.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5jJ2lteVWbf5","colab_type":"code","colab":{}},"source":["import argparse\n","import glob\n","import json\n","import logging\n","import os\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8qgo3IEqWfMw","colab_type":"code","outputId":"e266967d-99c6-4693-f342-b58c457b7d6e","executionInfo":{"status":"ok","timestamp":1581083526098,"user_tz":-480,"elapsed":25465,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    get_linear_schedule_with_warmup,\n","    BertConfig,\n","    BertModel,\n","    BertPreTrainedModel,\n","    BertTokenizer,)"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"QYZ4xpD_WhRd","colab_type":"code","colab":{}},"source":["from transformers import glue_compute_metrics as compute_metrics\n","from transformers import glue_convert_examples_to_features as convert_examples_to_features\n","from transformers import glue_output_modes as output_modes\n","from transformers import glue_processors as processors"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTtk32NRWi8O","colab_type":"code","colab":{}},"source":["from transformers.data.processors.utils import InputExample, DataProcessor"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hA7Pdr3W5QJ","colab_type":"code","colab":{}},"source":["try:\n","    from torch.utils.tensorboard import SummaryWriter   #version 1.14 or higher\n","except ImportError:\n","    from tensorboardX import SummaryWriter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTXCX0MMW8Le","colab_type":"code","colab":{}},"source":["import code\n","import os\n","import pickle\n","from sklearn.metrics.pairwise import cosine_similarity\n","cosine=nn.CosineSimilarity(dim=1,eps=1e-6) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJNVdqCXW_ai","colab_type":"code","outputId":"9738e95d-196b-4f8f-8126-ad692e99efcd","executionInfo":{"status":"ok","timestamp":1581083586651,"user_tz":-480,"elapsed":85983,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KEPwCq-XXB6P","colab_type":"code","colab":{}},"source":["logger=logging.getLogger(__name__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oKI9vl9sXDoR","colab_type":"code","colab":{}},"source":["MODEL_CLASSES={\n","    \"bert\":(BertConfig,BertTokenizer),\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oKoG6Iu_XE8g","colab_type":"code","colab":{}},"source":["def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jt29G8VeXJ16","colab_type":"code","colab":{}},"source":["class FAQProcessor(DataProcessor):\n","    def get_data_from_file(self, file_dir):\n","        # 从csv文件读入原句和正例\n","        train_df = pd.read_csv(file_dir, sep=\"\\t\")\n","        self.candidate_title = train_df[\"title\"].tolist()\n","        self.candidate_reply = train_df[\"reply\"].tolist()\n","        self.candidate_translated = train_df[\"translated\"].tolist()\n","\n","\n","    def create_train_data(self):\n","        #生成训练数据集\n","        return self._create_examples(self.candidate_title, \"original\"),\\\n","                self._create_examples(self.candidate_translated, \"pos\")\n","\n","               \n","\n","    def _create_examples(self, lines, set_type):\n","        \"\"\"制作example\"\"\"\n","        examples = []\n","        for (i, line) in enumerate(lines):\n","            guid = \"%s-%s\" % (set_type, i)\n","            examples.append(InputExample(guid=guid, text_a=line, text_b=None, label=1))\n","        return examples\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CbBCKjQw9xfY","colab_type":"code","colab":{}},"source":["class BertForFAQHinge(BertPreTrainedModel):\n","    def __init__(self,config):\n","        super(BertPreTrainedModel,self).__init__(config)\n","        self.num_labels=config.num_labels\n","            \n","        self.bert=BertModel(config)\n","        self.dropout=nn.Dropout(config.hidden_dropout_prob)\n","        \n","        self.init_weights()\n","    \n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","        outputs=self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","        )   # 输出是 sequence_output, pooled_output, (hidden_states), (attentions)\n","        \n","        sequence_output, pooled_output=outputs[:2]\n","        # 1. CLS token\n","        output=pooled_output\n","        # 2. MEAN sequence\n","        #output=torch.mean(sequence_output,dim=1)\n","        # 3. MAX sequence\n","        #output=torch.max(sequence_output,dim=1)[0]\n","        \n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFPAfew-9M46","colab_type":"code","colab":{}},"source":["# 只做单个eval,所以inputs只有一个\n","def evaluate(args,model,eval_dataset):\n","    outputs=[]\n","\n","    \n","    eval_sampler =RandomSampler(eval_dataset)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args[\"batch_size\"])\n","    \n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args[\"batch_size\"])\n","    eval_loss=0.0\n","    nb_eval_steps=0\n","    preds=None\n","    out_label_ids=None\n","    \n","    for batch in tqdm(eval_dataloader,desc=\"Evaluating\"):\n","        model.eval()\n","        batch=tuple(t.to(args[\"device\"]) for t in batch)\n","        \n","        with torch.no_grad():\n","            inputs={\"input_ids\": batch[0], \"attention_mask\": batch[1], \"token_type_ids\": batch[2]} \n","            output=model(**inputs)\n","            \n","            outputs.append(output)\n","\n","  \n","    return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6dhJfk1QGIO","colab_type":"code","colab":{}},"source":["def train(args,train_dataset,model,processor,tokenizer):\n","    no_decay=[\"bias\",\"LayerNorm.weight\"]\n","    optimizer_grouped_parameters=[\n","        {\n","            \"params\":[p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\":args[\"weight_decay\"],\n","\n","        },\n","        {\n","            \"params\": [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\":0.0\n","        },\n","    ]\n","\n","    \n","    t_total=len(train_dataset)//args[\"gradient_accumulation_steps\"] * args[\"num_train_epochs\"]\n","    optimizer=AdamW(optimizer_grouped_parameters,lr=args[\"learning_rate\"],eps=args[\"adam_epsilon\"])\n","    # bert里的小技巧, bert里的learning rate是不断变化的,先往上升,再往下降,这个scheduler就是用来设置这个\n","    scheduler=get_linear_schedule_with_warmup(\n","        optimizer,num_warmup_steps=args[\"warmup_steps\"],num_training_steps=t_total\n","        )\n","    \n","    \n","    \n","    # *********************\n","    logger.info(\"*****Running training*****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args[\"num_train_epochs\"])\n","\n","\n","    epochs_trained=0\n","    global_step=0\n","    steps_trained_in_current_epoch=0\n","\n","    tr_loss,logging_loss=0.0,0.0\n","    model.zero_grad()\n","    train_iterator=trange(epochs_trained,args[\"num_train_epochs\"],desc=\"Epoch\",disable=False)\n","\n","    set_seed()\n","\n","    for k in train_iterator: #共5个epoch\n","    \n","        #train_sampler=RandomSampler(train_dataset)\n","        # 因为原句和正例没有匹配打包,所以这里只能用SequentialSampler, 不能随机\n","        train_sampler=SequentialSampler(train_dataset)\n","        train_dataloader=DataLoader(train_dataset,sampler=train_sampler,batch_size=args[\"batch_size\"])\n","        epoch_iterator=tqdm(train_dataloader,desc=\"Iteration\",disable=False)\n","\n","        for step,batch in enumerate(epoch_iterator): #每个epoch里555次iteration\n","            if steps_trained_in_current_epoch>0:\n","                steps_traned_in_current_epoch-=1\n","                continue\n","\n","            model.train()\n","            batch=tuple(t.to(args[\"device\"]) for t in batch)\n","            original_inputs={\"input_ids\":batch[0],\"attention_mask\":batch[1],\"token_type_ids\":batch[2]}\n","            pos_inputs={\"input_ids\":batch[3],\"attention_mask\":batch[4],\"token_type_ids\":batch[5]}\n","\n","            #根据args传进来的模式, 生成不同的embedding\n","            original_outputs=model(**original_inputs)\n","            pos_outputs=model(**pos_inputs)\n","\n","            # original_outputs,   size是32*768,type是torch.Tensor \n","\n","\n","\n","            #这里生成negative example\n","            neg_outputs=[]\n","            ''' \n","            针对每个原句,在这个batch里找一个负例\n","            比如这个batch里有32条句子, 我就拿里面每一条句子和其他31条句子比对,找出分数最高的一个作为负例.\n","            当然因为我把32条句子都整合在candidate_embeddings里面,所以每个句子会和它自己比较. 因此有个for循环\n","            找到第一个和原句不同的句子即可.\n","            '''\n","\n","            for original_embedding in original_outputs:\n","                scores = cosine(original_embedding.unsqueeze(0), original_outputs).detach().cpu().numpy()\n","                top3_indices = scores.argsort()[-3:][::-1]\n","                # 找到第一个和原句不同的句子作为负例,因为分数最高的可能是句子自己\n","                for index in top3_indices:\n","                    if not torch.all(original_embedding==original_outputs[index]):\n","                        neg_outputs.append(original_outputs[index])\n","                        break\n","\n","            neg_outputs=torch.stack([c for c in neg_outputs])\n","\n","            pos_score=cosine(original_outputs,pos_outputs)\n","            neg_score=cosine(original_outputs,neg_outputs)\n","\n","            loss=-(pos_score-neg_score-args[\"margin\"])\n","            loss[loss<0]=0\n","            loss=torch.mean(loss)\n","            loss.backward()\n","\n","            tr_loss+=loss.item()\n","            if (step+1)%args[\"gradient_accumulation_steps\"]==0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(),args[\"max_grad_norm\"])\n","\n","                optimizer.step()\n","                scheduler.step()\n","                model.zero_grad()\n","                global_step+=1\n","\n","        logger.info(\"average loss:\" +str(tr_loss/global_step))\n","\n","\n","    return global_step,tr_loss/global_step"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U39MogzVlp8K","colab_type":"code","colab":{}},"source":["def load_examples(args, tokenizer, processor):\n","\n","    original_data,pos_data= processor.create_train_data()\n"," \n","    # Load data features from cache or dataset file\n","    logger.info(\"Creating features from dataset file at %s\", args[\"data_dir\"])\n","\n","    original_features = convert_examples_to_features(\n","        original_data,\n","        tokenizer,\n","        label_list=[1],\n","        output_mode=\"classification\",\n","        max_length=args[\"max_seq_length\"],\n","        pad_on_left=bool(args[\"model_type\"] in [\"xlnet\"]),  # pad on the left for xlnet\n","        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","        pad_token_segment_id=4 if args[\"model_type\"] in [\"xlnet\"] else 0,\n","    )\n","    pos_features = convert_examples_to_features(\n","        pos_data,\n","        tokenizer,\n","        label_list=[1],\n","        output_mode=\"classification\",\n","        max_length=args[\"max_seq_length\"],\n","        pad_on_left=bool(args[\"model_type\"] in [\"xlnet\"]),  # pad on the left for xlnet\n","        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","        pad_token_segment_id=4 if args[\"model_type\"] in [\"xlnet\"] else 0,\n","    )\n","\n","    # Convert to Tensors and build dataset\n","    original_input_ids = torch.tensor([f.input_ids for f in original_features], dtype=torch.long)\n","    original_attention_mask = torch.tensor([f.attention_mask for f in original_features], dtype=torch.long)\n","    original_token_type_ids = torch.tensor([f.token_type_ids for f in original_features], dtype=torch.long)\n","\n","    pos_input_ids = torch.tensor([f.input_ids for f in pos_features], dtype=torch.long)\n","    pos_attention_mask = torch.tensor([f.attention_mask for f in pos_features], dtype=torch.long)\n","    pos_token_type_ids = torch.tensor([f.token_type_ids for f in pos_features], dtype=torch.long)\n","\n","\n","    if args[\"do_train\"]:\n","        dataset = TensorDataset(original_input_ids, original_attention_mask, original_token_type_ids,\\\n","                                pos_input_ids, pos_attention_mask, pos_token_type_ids)\n","    elif args[\"do_eval\"]:\n","        dataset = TensorDataset(original_input_ids, original_attention_mask, original_token_type_ids)\n","    else:\n","        print(\"Something wrong in load_examples function!!!!\")\n","\n","    return dataset, processor.candidate_title, processor.candidate_reply"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWwfqNim0y0v","colab_type":"code","colab":{}},"source":["def main(args):\n","        \n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n","    )\n","    \n","    \n","    set_seed()\n","    model_type = args[\"model_type\"]\n","    \n","    \n","    config_class, tokenizer_class = MODEL_CLASSES[model_type]\n","    model_class=BertForFAQHinge\n","\n","    config = config_class.from_pretrained(\n","        args[\"config_name\"],\n","        finetuning_task=args[\"task_name\"], \n","        cache_dir=None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args[\"tokenizer_name\"],\n","        do_lower_case=True,\n","        cache_dir=None,\n","    )\n","    model = model_class.from_pretrained(\n","        args[\"model_name_or_path\"],\n","        from_tf=bool(\".ckpt\" in args[\"model_name_or_path\"]),\n","        config=config,\n","        cache_dir=None,\n","    )\n","    \n","    \n","    model.to(args[\"device\"])\n","    \n","    logger.info(\"Training/evaluation parameters %s\", args)\n","    \n","    processor=FAQProcessor()\n","    processor.get_data_from_file(args[\"data_dir\"])\n","    \n","    dataset,candidate_title,candidate_reply=load_examples(args,tokenizer,processor)\n","    \n","    \n","    if args[\"do_train\"]:\n","        train(args,dataset,model,processor,tokenizer)\n","        if not os.path.exists(args[\"output_dir\"]):\n","            os.makedirs(args[\"output_dir\"])\n","        \n","        logger.info(\"Saving model checkpoint to %s\", args[\"output_dir\"])\n","        \n","        model.save_pretrained(args[\"output_dir\"])\n","        tokenizer.save_pretrained(args[\"output_dir\"])\n","        \n","        torch.save(args,os.path.join(args[\"output_dir\"],\"training_args.bin\"))\n","        \n","        model=model_class.from_pretrained(args[\"output_dir\"])\n","        tokenizer=tokenizer_class.from_pretrained(args[\"output_dir\"])\n","        model.to(args[\"device\"])\n","    \n","    \n","    if args[\"do_eval\"]:\n","        # load dataset\n","        if not os.path.exists(\"/content/drive/My Drive/hinge_embeddings.pkl\"):\n","            logger.info(\"Training/evaluation parameters %s\", args)\n","\n","            eval_dataset, candidate_title, candidate_reply = load_examples(args, tokenizer,processor)\n","        \n","            outputs = evaluate(args, model, eval_dataset)\n","        \n","            #把所有candidates embedding拼起来, 每个pooled_output是32, 拼起来一共 18677*768\n","            candidate_embeddings = torch.cat([o.cpu().data for o in outputs]).numpy()\n","\n","            with open(\"/content/drive/My Drive/hinge_embeddings.pkl\", \"wb\") as fout:\n","                pickle.dump([candidate_title, candidate_reply, candidate_embeddings], fout)\n","\n","                \n","        else:\n","            with open(\"/content/drive/My Drive/hinge_embeddings.pkl\", \"rb\") as fin:\n","                candidate_title, candidate_reply, candidate_embeddings = pickle.load(fin)\n","\n","\n","        while True:\n","            title = input(\"你的问题是？\\n\")\n","            if len(title.strip()) == 0:\n","                continue\n","            \n","            examples = [InputExample(guid=0, text_a=title, text_b=None, label=1)]\n","            features = convert_examples_to_features(\n","                examples,\n","                tokenizer,\n","                label_list=[1],\n","                output_mode=\"classification\",\n","                max_length=args[\"max_seq_length\"],\n","                pad_on_left=bool(args[\"model_type\"] in [\"xlnet\"]),  # pad on the left for xlnet\n","                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","                pad_token_segment_id=4 if args[\"model_type\"] in [\"xlnet\"] else 0,\n","            )\n","\n","            # Convert to Tensors and build dataset\n","            all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","            all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","            all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n","    \n","            dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids)\n","            outputs = evaluate(args, model, dataset)\n","            title_embedding = torch.cat([o.cpu().data for o in outputs]).numpy()\n","\n","            scores = cosine_similarity(title_embedding, candidate_embeddings)[0]\n","            top5_indices = scores.argsort()[-5:][::-1]\n","\n","            for index in top5_indices:\n","                print(\"可能的答案，参考问题：\" + candidate_title[index] + \"\\t答案：\" + candidate_reply[index] + \"\\t得分：\" + str(scores[index]))\n","                print()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PEAi_FaB8GA4","colab_type":"code","colab":{}},"source":["args_train={\n","    \"model_type\":\"bert\",\n","    \"data_dir\": \"/content/drive/My Drive/Data/preprocessed.csv\",\n","    \"output_dir\":\"/content/drive/My Drive/cos_hinge_models/\",\n","    \"model_name_or_path\": \"/content/drive/My Drive/chinese_wwm_ext_pytorch/\",\n","    \"config_name\": \"/content/drive/My Drive/chinese_wwm_ext_pytorch/#\",\n","    \"tokenizer_name\": \"/content/drive/My Drive/chinese_wwm_ext_pytorch/\",\n","    \"do_train\":True,\n","    \"do_eval\":False,\n","    \"evaluate_during_training\":False,\n","    \"do_lower_case\":False,\n","    \"per_gpu_train_batch_size\":32,\n","    \"per_gpu_eval_batch_size\":32,\n","    \"batch_size\":32,\n","    \"gradient_accumulation_steps\":1,\n","    \"learning_rate\":2e-5,\n","    \"adam_epsilon\":1e-8,\n","    \"max_grad_norm\":1.0,\n","    \"weight_decay\":0.0,\n","    \"max_grad_norm\":1.0,\n","    \"max_seq_length\":128,\n","    \"weight_decay\":0.0,\n","    \"num_train_epochs\":10,\n","    \"device\":\"cpu\",\n","    \"margin\":5,\n","    \"warmup_steps\":0,\n","    \"task_name\":\"\",\n","    \n","}\n","\n","if torch.cuda.is_available():\n","    args_train[\"device\"]=\"cuda\"\n","\n","\n","#main(args_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uc3JQG36OMnj","colab_type":"code","colab":{}},"source":["args_eval={\n","    \"model_type\":\"bert\",\n","    \"data_dir\": \"/content/drive/My Drive/Data/preprocessed.csv\",\n","    \"output_dir\":\"/content/drive/My Drive/cos_hinge_models/\",\n","    \"model_name_or_path\": \"/content/drive/My Drive/cos_hinge_models/\",\n","    \"config_name\": \"/content/drive/My Drive/cos_hinge_models/\",\n","    \"tokenizer_name\": \"/content/drive/My Drive/cos_hinge_models/\",\n","    \"do_train\":False,\n","    \"do_eval\":True,\n","    \"evaluate_during_training\":False,\n","    \"do_lower_case\":False,\n","    \"per_gpu_train_batch_size\":32,\n","    \"per_gpu_eval_batch_size\":32,\n","    \"batch_size\":32,\n","    \"gradient_accumulation_steps\":1,\n","    \"learning_rate\":2e-5,\n","    \"adam_epsilon\":1e-8,\n","    \"max_grad_norm\":1.0,\n","    \"weight_decay\":0.0,\n","    \"max_grad_norm\":1.0,\n","    \"max_seq_length\":128,\n","    \"weight_decay\":0.0,\n","    \"num_train_epochs\":10,\n","    \"device\":\"cpu\",\n","    \"margin\":5,\n","    \"warmup_steps\":0,\n","    \"task_name\":\"\",\n","    \n","    \n","}\n","\n","if torch.cuda.is_available():\n","    args_eval[\"device\"]=\"cuda\"\n","\n","\n","#main(args_eval)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyOsPA7d8Z_D","colab_type":"code","colab":{}},"source":["def mean_reciprocal_rank(rs):\n","    rs = (np.asarray(r).nonzero()[0] for r in rs)\n","    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n","\n","\n","def evaluate_mmr(args):\n","    # load model\n","    set_seed()\n","    task_name = \"\"\n","    model_type = args[\"model_type\"]\n","    \n","    \n","    config_class, tokenizer_class = MODEL_CLASSES[model_type]\n","    model_class=BertForFAQHinge\n","    \n","    config = config_class.from_pretrained(\n","        args[\"config_name\"],\n","        finetuning_task=task_name, \n","        cache_dir=None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args[\"tokenizer_name\"],\n","        do_lower_case=True,\n","        cache_dir=None,\n","    )\n","    model = model_class.from_pretrained(\n","        args[\"model_name_or_path\"],\n","        from_tf=bool(\".ckpt\" in args[\"model_name_or_path\"]),\n","        config=config,\n","        cache_dir=None,\n","    )\n","    model.to(args[\"device\"])\n","    \n","    \n","    \n","    # load candidate embeddings\n","    with open(\"/content/drive/My Drive/hinge_embeddings_cls.pkl\", \"rb\") as fin:\n","        candidate_title, candidate_reply, candidate_embeddings = pickle.load(fin)\n","    \n","    \n","    # load test data\n","    df=pd.read_csv(args[\"data_dir\"])\n","    questions=df[\"question\"].tolist()\n","    matched_questions=df[\"title\"].tolist()\n","    matched_questions_index = []\n","    for q in matched_questions:\n","        flg = False\n","        for i, _q in enumerate(candidate_title):\n","            if q == _q:\n","                matched_questions_index.append([i])\n","                flg = True\n","                break\n","        if flg == False:\n","            matched_questions_index.append([-1])\n","    \n","    matched_questions_index = np.asarray(matched_questions_index)\n","    \n","    \n","    #convert questions in test data to BERT input\n","    examples = [InputExample(guid=0, text_a=title, text_b=None, label=1) for title in questions]\n","\n","    \n","    features = convert_examples_to_features(\n","        examples,\n","        tokenizer,\n","        label_list=[1],\n","        output_mode=\"classification\",\n","        max_length=args[\"max_seq_length\"],\n","        pad_on_left=bool(args[\"model_type\"] in [\"xlnet\"]),  # pad on the left for xlnet\n","        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","        pad_token_segment_id=4 if args[\"model_type\"] in [\"xlnet\"] else 0,\n","    )\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n","\n","\n","    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids)\n","    sequence_outputs = evaluate(args, model, dataset)\n","    question_embedding = torch.cat([o.cpu() for o in sequence_outputs]).numpy()\n","\n","\n","    \n","    \n","    scores = cosine_similarity(question_embedding, candidate_embeddings)\n","    sorted_indices = scores.argsort()[:, ::-1]#[-5:][::-1]\n","    # code.interact(local=locals())\n","    mmr = mean_reciprocal_rank(sorted_indices==matched_questions_index)\n","    print(\"mean reciprocal rank: {}\".format(mmr))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFGDyeXw8laQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"5ae54cf4-f95f-4227-c737-ee107b3116c6","executionInfo":{"status":"ok","timestamp":1581084832992,"user_tz":-480,"elapsed":11461,"user":{"displayName":"たまものまえ","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDU-25fRfeKXg0eAeXT2v1GYZ7Ree5bkAUIuXxJ=s64","userId":"07646311357530751070"}}},"source":["args={\n","    \"model_type\":\"bert\",\n","    \"data_dir\": \"/content/drive/My Drive/Data/lawzhidao_evaluate.csv\",\n","    \"model_name_or_path\": \"/content/drive/My Drive/cos_hinge_models/\",\n","    \"config_name\": \"/content/drive/My Drive/cos_hinge_models/\",\n","    \"tokenizer_name\": \"/content/drive/My Drive/cos_hinge_models/\",\n","    \"do_train\":False,\n","    \"do_eval\":False,\n","    \"evaluate_during_training\":False,\n","    \"do_lower_case\":False,\n","    \"per_gpu_train_batch_size\":32,\n","    \"per_gpu_eval_batch_size\":32,\n","    \"batch_size\":32,\n","    \"gradient_accumulation_steps\":1,\n","    \"learning_rate\":5e-5,\n","    \"adam_epsilon\":1e-8,\n","    \"max_grad_norm\":1.0,\n","    \"weight_decay\":0.0,\n","    \"max_grad_norm\":1.0,\n","    \"max_seq_length\":128,\n","    \"device\":\"cpu\",\n","    \n","    \n","}\n","\n","\n","if torch.cuda.is_available():\n","    args[\"device\"]=\"cuda\"\n","    \n","    \n","evaluate_mmr(args)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Evaluating: 100%|██████████| 2/2 [00:00<00:00,  6.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["mean reciprocal rank: 0.00023563829506073443\n"],"name":"stdout"}]}]}